# Comparative Analysis: Traditional Fine-tuning vs. LoRA for Microsoft's GIT-Base 

This project performs a systematic comparison of **Traditional Fine-tuning** and **Low-Rank Adaptation (LoRA)** for large language models on downstream classification tasks.
 It benchmarks performance, memory, and computational efficiency across tasks and model scales.

