{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeet1912/florence-2_ablationStudy/blob/main/code/mimic_cxr_ablation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yrq0KZJpo5xl",
        "outputId": "388f92b5-bb00-44cb-c024-9db40a941c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting gcsfs\n",
            "  Downloading gcsfs-2025.7.0-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (3.12.15)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (4.4.2)\n",
            "Collecting fsspec==2025.7.0 (from gcsfs)\n",
            "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.19.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gcsfs) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.20.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gcsfs) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.3.1)\n",
            "Downloading gcsfs-2025.7.0-py2.py3-none-any.whl (36 kB)\n",
            "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, gcsfs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: gcsfs\n",
            "    Found existing installation: gcsfs 2025.3.0\n",
            "    Uninstalling gcsfs-2025.3.0:\n",
            "      Successfully uninstalled gcsfs-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2025.7.0 gcsfs-2025.7.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3e5152f2d45141ac859f641b83fa08a8",
              "pip_warning": {
                "packages": [
                  "fsspec",
                  "gcsfs"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rQ59GbBNwvj",
        "outputId": "2b9119d7-cecd-49c3-f171-a50239a08917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade google-cloud-storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34c7An9Rwrb5",
        "outputId": "09b05785-d28c-4822-86d8-22841eae046a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n",
            "Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install iterative-stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn6Qdg0xyJo-",
        "outputId": "bb054241-d43c-4ee8-9f02-4ed50838fc1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.7.0 requires fsspec==2025.7.0, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.23.4 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed evaluate-0.4.5 fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIP9ScA13mCq",
        "outputId": "a3af0395-296d-43c2-f983-f311b2653b9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.54.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.9.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.34.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.8.3)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6FUx7DyKtZD",
        "outputId": "942204b6-695a-4bae-f39a-08d09b6a0ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import gcsfs\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import io\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import itertools\n",
        "from evaluate import load\n",
        "import gc\n",
        "import time\n",
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"colorblind\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLG7hYueT1Lz"
      },
      "outputs": [],
      "source": [
        "class CXR(Dataset):\n",
        "  def __init__(self, dataframe, processor, max_length):\n",
        "    super().__init__()\n",
        "    self.dataframe = dataframe.reset_index(drop=True)\n",
        "    self.processor = processor\n",
        "    self.max_length = max_length\n",
        "    self.prompt = \"List pathalogical findings for this chest X-ray:\"\n",
        "    self.storage_client = storage.Client(project='ablation-study')\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "\n",
        "  def _loadImage(self,subject_id, study_id, dicom_id):\n",
        "    try:\n",
        "      bucket_name = \"mimic-cxr-jpg-2.1.0.physionet.org\"\n",
        "      image_path = f\"files/p{subject_id[:2]}/p{subject_id}/s{study_id}/{dicom_id}.jpg\"\n",
        "      bucket = self.storage_client.bucket(bucket_name, user_project='ablation-study')\n",
        "      blob = bucket.blob(image_path)\n",
        "      image_bytes = blob.download_as_bytes()\n",
        "      image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
        "      return image\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading image {image_path}: {str(e)}\")\n",
        "      return None # Return None if image loading fails\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    row = self.dataframe.iloc[index]\n",
        "    miniReport = str(row['mini_report'])\n",
        "    subject = str(row['subject_id'])\n",
        "    study = str(row['study_id'])\n",
        "    dicom = str(row['dicom_id'])\n",
        "    image = self._loadImage(subject_id=subject,study_id=study,dicom_id=dicom)\n",
        "    inputs = self.processor(images=image, text=self.prompt, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                            truncation=True, max_length=self.max_length)\n",
        "    labels = self.processor.tokenizer(miniReport, return_tensors=\"pt\", padding=\"max_length\",\n",
        "                                      truncation=True, max_length=self.max_length)[\"input_ids\"]\n",
        "\n",
        "    return {\n",
        "      \"pixel_values\": inputs[\"pixel_values\"],  # Shape: [1, 3, H, W]\n",
        "      \"input_ids\": inputs[\"input_ids\"],        # Shape: [1, max_length]\n",
        "      \"attention_mask\": inputs[\"attention_mask\"],  # Shape: [1, max_length]\n",
        "      \"labels\": labels                         # Shape: [1, max_length]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZDMS387yud0"
      },
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ymi9N_y10l",
        "outputId": "696dc64e-8539-4958-e4a8-6f446e076e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of mini-reports: 92\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv('/content/drive/MyDrive/ds677_ablationStudy/train_split.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/ds677_ablationStudy/test_split.csv')\n",
        "val_df = pd.read_csv('/content/drive/MyDrive/ds677_ablationStudy/val_split.csv')\n",
        "\n",
        "max_length = max(len(processor.tokenizer.encode(report)) for report in train_df['mini_report'])\n",
        "print(f\"Max length of mini-reports: {max_length}\")\n",
        "\n",
        "train_dataset = CXR(train_df, processor, max_length)\n",
        "val_dataset = CXR(val_df, processor, max_length)\n",
        "test_dataset = CXR(test_df, processor, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXcl1XeuzFfx",
        "outputId": "b9da40a8-1a5d-4548-bb1b-1a16fbd65fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n",
            "torch.Size([1, 92])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0]['pixel_values'].shape)\n",
        "print(train_dataset[0]['input_ids'].shape)\n",
        "print(train_dataset[0]['attention_mask'].shape)\n",
        "print(train_dataset[0]['labels'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYIiBhE8zPLC"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/ds677_ablationStudy/results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYyw_Wfe03Fp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJfqR-5y8qKq",
        "outputId": "70e102c8-4112-452f-f1a3-6fbc3cabebc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbW4bQ9VJtyW"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    pixel_values = torch.cat([item[\"pixel_values\"] for item in batch], dim=0)  # [batch_size, 3, H, W]\n",
        "    input_ids = torch.cat([item[\"input_ids\"] for item in batch], dim=0)        # [batch_size, max_length]\n",
        "    attention_mask = torch.cat([item[\"attention_mask\"] for item in batch], dim=0)  # [batch_size, max_length]\n",
        "    labels = torch.cat([item[\"labels\"] for item in batch], dim=0)               # [batch_size, max_length]\n",
        "\n",
        "    #print(f\"Batch shapes: pixel_values={pixel_values.shape}, input_ids={input_ids.shape}, \"\n",
        "    #      f\"attention_mask={attention_mask.shape}, labels={labels.shape}\")\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFTGc_BMLwrO"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71O_HHP3L1pv",
        "outputId": "4a44642e-73ab-46d6-e8b2-83f61a7b3a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "git\n",
            "git.embeddings\n",
            "git.embeddings.word_embeddings\n",
            "git.embeddings.position_embeddings\n",
            "git.embeddings.LayerNorm\n",
            "git.embeddings.dropout\n",
            "git.image_encoder\n",
            "git.image_encoder.vision_model\n",
            "git.image_encoder.vision_model.embeddings\n",
            "git.image_encoder.vision_model.embeddings.patch_embedding\n",
            "git.image_encoder.vision_model.embeddings.position_embedding\n",
            "git.image_encoder.vision_model.pre_layrnorm\n",
            "git.image_encoder.vision_model.encoder\n",
            "git.image_encoder.vision_model.encoder.layers\n",
            "git.image_encoder.vision_model.encoder.layers.0\n",
            "git.image_encoder.vision_model.encoder.layers.0.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.0.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.0.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.0.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.0.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.0.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.0.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.0.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.0.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.0.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.0.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.1\n",
            "git.image_encoder.vision_model.encoder.layers.1.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.1.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.1.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.1.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.1.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.1.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.1.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.1.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.1.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.1.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.1.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.2\n",
            "git.image_encoder.vision_model.encoder.layers.2.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.2.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.2.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.2.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.2.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.2.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.2.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.2.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.2.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.2.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.2.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.3\n",
            "git.image_encoder.vision_model.encoder.layers.3.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.3.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.3.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.3.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.3.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.3.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.3.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.3.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.3.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.3.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.3.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.4\n",
            "git.image_encoder.vision_model.encoder.layers.4.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.4.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.4.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.4.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.4.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.4.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.4.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.4.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.4.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.4.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.4.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.5\n",
            "git.image_encoder.vision_model.encoder.layers.5.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.5.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.5.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.5.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.5.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.5.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.5.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.5.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.5.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.5.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.5.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.6\n",
            "git.image_encoder.vision_model.encoder.layers.6.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.6.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.6.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.6.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.6.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.6.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.6.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.6.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.6.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.6.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.6.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.7\n",
            "git.image_encoder.vision_model.encoder.layers.7.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.7.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.7.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.7.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.7.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.7.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.7.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.7.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.7.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.7.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.7.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.8\n",
            "git.image_encoder.vision_model.encoder.layers.8.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.8.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.8.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.8.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.8.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.8.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.8.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.8.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.8.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.8.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.8.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.9\n",
            "git.image_encoder.vision_model.encoder.layers.9.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.9.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.9.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.9.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.9.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.9.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.9.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.9.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.9.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.9.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.9.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.10\n",
            "git.image_encoder.vision_model.encoder.layers.10.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.10.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.10.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.10.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.10.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.10.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.10.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.10.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.10.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.10.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.10.layer_norm2\n",
            "git.image_encoder.vision_model.encoder.layers.11\n",
            "git.image_encoder.vision_model.encoder.layers.11.self_attn\n",
            "git.image_encoder.vision_model.encoder.layers.11.self_attn.k_proj\n",
            "git.image_encoder.vision_model.encoder.layers.11.self_attn.v_proj\n",
            "git.image_encoder.vision_model.encoder.layers.11.self_attn.q_proj\n",
            "git.image_encoder.vision_model.encoder.layers.11.self_attn.out_proj\n",
            "git.image_encoder.vision_model.encoder.layers.11.layer_norm1\n",
            "git.image_encoder.vision_model.encoder.layers.11.mlp\n",
            "git.image_encoder.vision_model.encoder.layers.11.mlp.activation_fn\n",
            "git.image_encoder.vision_model.encoder.layers.11.mlp.fc1\n",
            "git.image_encoder.vision_model.encoder.layers.11.mlp.fc2\n",
            "git.image_encoder.vision_model.encoder.layers.11.layer_norm2\n",
            "git.image_encoder.vision_model.post_layernorm\n",
            "git.encoder\n",
            "git.encoder.layer\n",
            "git.encoder.layer.0\n",
            "git.encoder.layer.0.attention\n",
            "git.encoder.layer.0.attention.self\n",
            "git.encoder.layer.0.attention.self.query\n",
            "git.encoder.layer.0.attention.self.key\n",
            "git.encoder.layer.0.attention.self.value\n",
            "git.encoder.layer.0.attention.self.dropout\n",
            "git.encoder.layer.0.attention.output\n",
            "git.encoder.layer.0.attention.output.dense\n",
            "git.encoder.layer.0.attention.output.LayerNorm\n",
            "git.encoder.layer.0.attention.output.dropout\n",
            "git.encoder.layer.0.intermediate\n",
            "git.encoder.layer.0.intermediate.dense\n",
            "git.encoder.layer.0.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.0.output\n",
            "git.encoder.layer.0.output.dense\n",
            "git.encoder.layer.0.output.LayerNorm\n",
            "git.encoder.layer.0.output.dropout\n",
            "git.encoder.layer.1\n",
            "git.encoder.layer.1.attention\n",
            "git.encoder.layer.1.attention.self\n",
            "git.encoder.layer.1.attention.self.query\n",
            "git.encoder.layer.1.attention.self.key\n",
            "git.encoder.layer.1.attention.self.value\n",
            "git.encoder.layer.1.attention.self.dropout\n",
            "git.encoder.layer.1.attention.output\n",
            "git.encoder.layer.1.attention.output.dense\n",
            "git.encoder.layer.1.attention.output.LayerNorm\n",
            "git.encoder.layer.1.attention.output.dropout\n",
            "git.encoder.layer.1.intermediate\n",
            "git.encoder.layer.1.intermediate.dense\n",
            "git.encoder.layer.1.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.1.output\n",
            "git.encoder.layer.1.output.dense\n",
            "git.encoder.layer.1.output.LayerNorm\n",
            "git.encoder.layer.1.output.dropout\n",
            "git.encoder.layer.2\n",
            "git.encoder.layer.2.attention\n",
            "git.encoder.layer.2.attention.self\n",
            "git.encoder.layer.2.attention.self.query\n",
            "git.encoder.layer.2.attention.self.key\n",
            "git.encoder.layer.2.attention.self.value\n",
            "git.encoder.layer.2.attention.self.dropout\n",
            "git.encoder.layer.2.attention.output\n",
            "git.encoder.layer.2.attention.output.dense\n",
            "git.encoder.layer.2.attention.output.LayerNorm\n",
            "git.encoder.layer.2.attention.output.dropout\n",
            "git.encoder.layer.2.intermediate\n",
            "git.encoder.layer.2.intermediate.dense\n",
            "git.encoder.layer.2.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.2.output\n",
            "git.encoder.layer.2.output.dense\n",
            "git.encoder.layer.2.output.LayerNorm\n",
            "git.encoder.layer.2.output.dropout\n",
            "git.encoder.layer.3\n",
            "git.encoder.layer.3.attention\n",
            "git.encoder.layer.3.attention.self\n",
            "git.encoder.layer.3.attention.self.query\n",
            "git.encoder.layer.3.attention.self.key\n",
            "git.encoder.layer.3.attention.self.value\n",
            "git.encoder.layer.3.attention.self.dropout\n",
            "git.encoder.layer.3.attention.output\n",
            "git.encoder.layer.3.attention.output.dense\n",
            "git.encoder.layer.3.attention.output.LayerNorm\n",
            "git.encoder.layer.3.attention.output.dropout\n",
            "git.encoder.layer.3.intermediate\n",
            "git.encoder.layer.3.intermediate.dense\n",
            "git.encoder.layer.3.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.3.output\n",
            "git.encoder.layer.3.output.dense\n",
            "git.encoder.layer.3.output.LayerNorm\n",
            "git.encoder.layer.3.output.dropout\n",
            "git.encoder.layer.4\n",
            "git.encoder.layer.4.attention\n",
            "git.encoder.layer.4.attention.self\n",
            "git.encoder.layer.4.attention.self.query\n",
            "git.encoder.layer.4.attention.self.key\n",
            "git.encoder.layer.4.attention.self.value\n",
            "git.encoder.layer.4.attention.self.dropout\n",
            "git.encoder.layer.4.attention.output\n",
            "git.encoder.layer.4.attention.output.dense\n",
            "git.encoder.layer.4.attention.output.LayerNorm\n",
            "git.encoder.layer.4.attention.output.dropout\n",
            "git.encoder.layer.4.intermediate\n",
            "git.encoder.layer.4.intermediate.dense\n",
            "git.encoder.layer.4.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.4.output\n",
            "git.encoder.layer.4.output.dense\n",
            "git.encoder.layer.4.output.LayerNorm\n",
            "git.encoder.layer.4.output.dropout\n",
            "git.encoder.layer.5\n",
            "git.encoder.layer.5.attention\n",
            "git.encoder.layer.5.attention.self\n",
            "git.encoder.layer.5.attention.self.query\n",
            "git.encoder.layer.5.attention.self.key\n",
            "git.encoder.layer.5.attention.self.value\n",
            "git.encoder.layer.5.attention.self.dropout\n",
            "git.encoder.layer.5.attention.output\n",
            "git.encoder.layer.5.attention.output.dense\n",
            "git.encoder.layer.5.attention.output.LayerNorm\n",
            "git.encoder.layer.5.attention.output.dropout\n",
            "git.encoder.layer.5.intermediate\n",
            "git.encoder.layer.5.intermediate.dense\n",
            "git.encoder.layer.5.intermediate.intermediate_act_fn\n",
            "git.encoder.layer.5.output\n",
            "git.encoder.layer.5.output.dense\n",
            "git.encoder.layer.5.output.LayerNorm\n",
            "git.encoder.layer.5.output.dropout\n",
            "git.visual_projection\n",
            "git.visual_projection.visual_projection\n",
            "git.visual_projection.visual_projection.0\n",
            "git.visual_projection.visual_projection.1\n",
            "output\n"
          ]
        }
      ],
      "source": [
        "for name, module in model.named_modules():\n",
        "  print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHRCKS4H8rSX"
      },
      "outputs": [],
      "source": [
        "# Define target modules for LoRA\n",
        "vision_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
        "text_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
        "\n",
        "# Ablation configurations\n",
        "ablation_configs = [\n",
        "    {\"name\": \"lora_vision_only\", \"target_modules\": vision_modules},\n",
        "    {\"name\": \"lora_text_only\", \"target_modules\": text_modules},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idBJse469tLQ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/ds677_ablationStudy/results/lora_target_modules\",\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=1e-5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_total_limit=1,  # Save only the best model\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    report_to=\"none\",\n",
        "    lr_scheduler_type=\"reduce_lr_on_plateau\",\n",
        "    optim=\"adamw_torch\",\n",
        "    label_names=[\"labels\"]\n",
        ")\n",
        "\n",
        "bleu = load(\"bleu\")\n",
        "\n",
        "def evaluate_model(model, test_loader, processor, max_length, device):\n",
        "    processor.tokenizer.padding_side = 'left'\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_samples = 0\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            batch_size_actual = batch[\"pixel_values\"].size(0)\n",
        "            test_loss += loss.item() * batch_size_actual\n",
        "            test_samples += batch_size_actual\n",
        "\n",
        "            pixel_values = batch[\"pixel_values\"]\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            generated_ids = model.generate(\n",
        "              pixel_values=pixel_values,\n",
        "              input_ids=input_ids,\n",
        "              max_new_tokens=50,  # Generate up to 50 new tokens\n",
        "              do_sample=False,    # Use greedy decoding for reproducible results\n",
        "              pad_token_id=processor.tokenizer.pad_token_id\n",
        "            )\n",
        "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            predictions.extend([text.replace(\"List pathalogical findings for this chest X-ray:\", \"\").strip() for text in generated_texts])\n",
        "\n",
        "            label_ids = batch[\"labels\"]\n",
        "            reference_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "            references.extend(reference_texts)\n",
        "\n",
        "    avg_test_loss = test_loss / test_samples\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "    return avg_test_loss, bleu_score[\"bleu\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "CnqGFYu8-TgW",
        "outputId": "9131b832-441b-4312-b813-57b16a55f57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running ablation experiment: lora_vision_only\n",
            "trainable params: 1,327,104 || all params: 177,946,170 || trainable%: 0.7458\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2236' max='4170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2236/4170 10:31:11 < 9:06:25, 0.06 it/s, Epoch 2.68/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.105000</td>\n",
              "      <td>8.740946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>8.681500</td>\n",
              "      <td>8.604371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2273' max='4170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2273/4170 10:41:04 < 8:55:29, 0.06 it/s, Epoch 2.72/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>9.105000</td>\n",
              "      <td>8.740946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>8.681500</td>\n",
              "      <td>8.604371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1040719069.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Train using Trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Extract losses from trainer logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2235\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2237\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2238\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2530\u001b[0m                 \u001b[0mupdate_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2531\u001b[0m                 \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mupdate_step\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_updates\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mremainder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2532\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2533\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m                     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_batch_samples\u001b[0;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[1;32m   5348\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5349\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5350\u001b[0;31m                 \u001b[0mbatch_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5352\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    577\u001b[0m                     \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m                 \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mcurrent_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-254339730.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'study_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdicom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dicom_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstudy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdicom_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdicom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     inputs = self.processor(images=image, text=self.prompt, return_tensors=\"pt\", padding=\"max_length\",\n\u001b[1;32m     33\u001b[0m                             truncation=True, max_length=self.max_length)\n",
            "\u001b[0;32m/tmp/ipython-input-254339730.py\u001b[0m in \u001b[0;36m_loadImage\u001b[0;34m(self, subject_id, study_id, dicom_id)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_project\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ablation-study'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mimage_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_as_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mdownload_as_bytes\u001b[0;34m(self, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, single_shot_download)\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0mstring_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             self._prep_and_do_download(\n\u001b[0m\u001b[1;32m   1531\u001b[0m                 \u001b[0mstring_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_prep_and_do_download\u001b[0;34m(self, file_obj, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry, single_shot_download, command)\u001b[0m\n\u001b[1;32m   4399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4400\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4401\u001b[0;31m             self._do_download(\n\u001b[0m\u001b[1;32m   4402\u001b[0m                 \u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 \u001b[0mfile_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36m_do_download\u001b[0;34m(self, transport, file_obj, download_url, headers, start, end, raw_download, timeout, checksum, retry, single_shot_download)\u001b[0m\n\u001b[1;32m   1092\u001b[0m                 \u001b[0mapi_request\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             ):\n\u001b[0;32m-> 1094\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_headers_from_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/_media/requests/download.py\u001b[0m in \u001b[0;36mconsume\u001b[0;34m(self, transport, timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_request_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_and_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriable_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/_media/requests/_request_helpers.py\u001b[0m in \u001b[0;36mwait_and_retry\u001b[0;34m(func, retry_strategy)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretry_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/_media/requests/download.py\u001b[0m in \u001b[0;36mretriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m                     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_query_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedia_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;31m# If a generation hasn't been specified, and this is the first response we get, let's record the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# A100 Resource Management Configuration\n",
        "class A100ResourceManager:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.initial_memory = None\n",
        "\n",
        "    def setup_memory_optimization(self):\n",
        "        \"\"\"Configure PyTorch for optimal A100 memory usage and speed\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            # Enable memory fraction to prevent OOM\n",
        "            torch.cuda.set_per_process_memory_fraction(0.90)  # Use 90% of GPU memory for speed\n",
        "\n",
        "            # Enable memory mapping for large models\n",
        "            torch.backends.cuda.matmul.allow_tf32 = True\n",
        "            torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "            # Enable optimized attention for speed (when supported)\n",
        "            try:\n",
        "                torch.backends.cuda.enable_flash_sdp(True)\n",
        "                torch.backends.cuda.enable_math_sdp(True)\n",
        "                torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "            except AttributeError:\n",
        "                # These may not be available in all PyTorch versions\n",
        "                print(\"Advanced SDP backends not available, using default attention\")\n",
        "\n",
        "            # Optimize memory allocation for speed\n",
        "            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256,expandable_segments:True,roundup_power2_divisions:16'\n",
        "\n",
        "            # Performance optimizations\n",
        "            os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'\n",
        "            os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async CUDA operations\n",
        "\n",
        "            # Set CUDA memory growth strategy\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Record initial memory state\n",
        "            self.initial_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "    def clear_memory(self, aggressive=False):\n",
        "        \"\"\"Comprehensive memory cleanup\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            if aggressive:\n",
        "                # Force garbage collection multiple times\n",
        "                for _ in range(3):\n",
        "                    gc.collect()\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # CPU memory cleanup\n",
        "        gc.collect()\n",
        "\n",
        "    def get_memory_stats(self):\n",
        "        \"\"\"Get detailed memory statistics\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "            cached = torch.cuda.memory_reserved() / 1024**2  # MB\n",
        "            max_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "\n",
        "            return {\n",
        "                \"allocated_mb\": allocated,\n",
        "                \"cached_mb\": cached,\n",
        "                \"max_allocated_mb\": max_allocated\n",
        "            }\n",
        "        return {\"allocated_mb\": 0, \"cached_mb\": 0, \"max_allocated_mb\": 0}\n",
        "\n",
        "    @contextmanager\n",
        "    def memory_managed_training(self):\n",
        "        \"\"\"Context manager for memory-safe training\"\"\"\n",
        "        try:\n",
        "            self.clear_memory(aggressive=True)\n",
        "            yield\n",
        "        finally:\n",
        "            self.clear_memory(aggressive=True)\n",
        "\n",
        "# Initialize resource manager\n",
        "resource_manager = A100ResourceManager()\n",
        "resource_manager.setup_memory_optimization()\n",
        "\n",
        "# High-performance training arguments optimized for speed\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/GIT AblationStratergy/ablationStudy/results/lora_target_modules\",\n",
        "    per_device_train_batch_size=64,  # Increased for better GPU utilization\n",
        "    per_device_eval_batch_size=64,   # Increased for faster evaluation\n",
        "    gradient_accumulation_steps=1,   # Reduced for faster updates\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,  # Increased learning rate for faster convergence\n",
        "    eval_strategy=\"no\",  # Disabled for speed - only evaluate at end\n",
        "    save_strategy=\"no\",  # Disabled intermediate saves for speed\n",
        "    logging_steps=50,    # Less frequent logging\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=False,  # Disabled for speed\n",
        "    report_to=\"none\",\n",
        "    lr_scheduler_type=\"constant\",  # Simpler scheduler for speed\n",
        "    optim=\"adamw_torch_fused\",  # Fused optimizer for A100\n",
        "    label_names=[\"labels\"],\n",
        "    # Performance optimization flags\n",
        "    dataloader_pin_memory=True,   # Enable for faster data transfer\n",
        "    dataloader_num_workers=4,     # Parallel data loading\n",
        "    remove_unused_columns=True,\n",
        "    bf16=True,  # Use BF16 instead of FP16 for A100 (faster and more stable)\n",
        "    tf32=True,  # Enable TF32 for even faster training on A100\n",
        "    gradient_checkpointing=False,\n",
        "    # Speed optimizations\n",
        "    dataloader_drop_last=True,    # Avoid partial batches\n",
        "    group_by_length=False,        # Disabled as it can slow down vision tasks\n",
        "    prediction_loss_only=True,    # Only compute loss, skip other metrics\n",
        "    disable_tqdm=False,           # Keep progress bar for monitoring\n",
        ")\n",
        "\n",
        "def configure_tokenizer_padding(processor):\n",
        "    \"\"\"Ensure consistent tokenizer padding configuration\"\"\"\n",
        "    processor.tokenizer.padding_side = 'left'\n",
        "    # Ensure pad token is properly set\n",
        "    if processor.tokenizer.pad_token is None:\n",
        "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
        "    # Ensure pad_token_id is set\n",
        "    if processor.tokenizer.pad_token_id is None:\n",
        "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
        "\n",
        "    print(f\"Tokenizer configuration:\")\n",
        "    print(f\"  - padding_side: {processor.tokenizer.padding_side}\")\n",
        "    print(f\"  - pad_token: '{processor.tokenizer.pad_token}'\")\n",
        "    print(f\"  - pad_token_id: {processor.tokenizer.pad_token_id}\")\n",
        "\n",
        "# Optimized evaluation function with memory management and fixed padding\n",
        "def evaluate_model_optimized(model, test_loader, processor, max_length, device):\n",
        "    \"\"\"Memory-optimized evaluation function with proper padding handling\"\"\"\n",
        "    # CRITICAL: Ensure left padding is set before any generation\n",
        "    configure_tokenizer_padding(processor)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_samples = 0\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Process in smaller chunks to avoid memory spikes\n",
        "    chunk_size = 16\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Split batch into smaller chunks if needed\n",
        "            batch_size = batch[\"pixel_values\"].size(0)\n",
        "\n",
        "            for chunk_start in range(0, batch_size, chunk_size):\n",
        "                chunk_end = min(chunk_start + chunk_size, batch_size)\n",
        "                chunk_batch = {k: v[chunk_start:chunk_end] for k, v in batch.items()}\n",
        "\n",
        "                outputs = model(**chunk_batch)\n",
        "                loss = outputs.loss\n",
        "                chunk_size_actual = chunk_batch[\"pixel_values\"].size(0)\n",
        "                test_loss += loss.item() * chunk_size_actual\n",
        "                test_samples += chunk_size_actual\n",
        "\n",
        "                # CRITICAL: Re-verify padding before generation (some operations can reset it)\n",
        "                if processor.tokenizer.padding_side != 'left':\n",
        "                    print(f\"WARNING: Padding side was reset to {processor.tokenizer.padding_side}, fixing...\")\n",
        "                    configure_tokenizer_padding(processor)\n",
        "\n",
        "                # Generate predictions in chunks\n",
        "                pixel_values = chunk_batch[\"pixel_values\"]\n",
        "                input_ids = chunk_batch[\"input_ids\"]\n",
        "\n",
        "                # Ensure input_ids are properly formatted for left padding\n",
        "                attention_mask = chunk_batch.get(\"attention_mask\", None)\n",
        "\n",
        "                generated_ids = model.generate(\n",
        "                    pixel_values=pixel_values,\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,  # Include attention mask\n",
        "                    max_new_tokens=92,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
        "                    use_cache=True  # Enable caching for better performance\n",
        "                )\n",
        "\n",
        "                generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "                predictions.extend([text.replace(\"List pathalogical findings for this chest X-ray:\", \"\").strip() for text in generated_texts])\n",
        "\n",
        "                label_ids = chunk_batch[\"labels\"]\n",
        "                reference_texts = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "                references.extend(reference_texts)\n",
        "\n",
        "                # Clear intermediate tensors\n",
        "                del chunk_batch, outputs, generated_ids\n",
        "\n",
        "            # Clean up batch tensors\n",
        "            del batch\n",
        "\n",
        "            # Periodic memory cleanup during evaluation\n",
        "            if i % 10 == 0:\n",
        "                torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
        "\n",
        "    avg_test_loss = test_loss / test_samples\n",
        "    bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "    return avg_test_loss, bleu_score[\"bleu\"]\n",
        "\n",
        "# Fixed high-performance trainer class for speed optimization\n",
        "class HighSpeedPaddingTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
        "        \"\"\"\n",
        "        Optimized training_step for maximum speed\n",
        "        \"\"\"\n",
        "        # Ensure tokenizer padding is correct before each training step\n",
        "        processing_class = getattr(self, 'processing_class', None)\n",
        "        if processing_class is None:\n",
        "            processing_class = getattr(self, 'tokenizer', None)\n",
        "\n",
        "        if processing_class and hasattr(processing_class, 'padding_side'):\n",
        "            if processing_class.padding_side != 'left':\n",
        "                processing_class.padding_side = 'left'\n",
        "\n",
        "        # Call parent's training_step with proper arguments\n",
        "        if num_items_in_batch is not None:\n",
        "            return super().training_step(model, inputs, num_items_in_batch)\n",
        "        else:\n",
        "            return super().training_step(model, inputs)\n",
        "\n",
        "    def _save_checkpoint(self, model, trial, metrics=None):\n",
        "        \"\"\"Skip checkpointing for speed during training\"\"\"\n",
        "        # Only save at the very end\n",
        "        if self.state.epoch >= self.args.num_train_epochs - 0.01:\n",
        "            super()._save_checkpoint(model, trial, metrics)\n",
        "\n",
        "    def log(self, logs, start_time=None):\n",
        "        \"\"\"Reduced logging for speed with correct method signature\"\"\"\n",
        "        # Only log every N steps to reduce I/O overhead\n",
        "        if self.state.global_step % 50 == 0:\n",
        "            # Call parent's log method with correct arguments\n",
        "            if start_time is not None:\n",
        "                super().log(logs, start_time)\n",
        "            else:\n",
        "                super().log(logs)\n",
        "\n",
        "# Main ablation study loop with optimized resource management\n",
        "results = []\n",
        "\n",
        "for config_idx, config in enumerate(ablation_configs):\n",
        "    print(f\"\\nRunning ablation experiment: {config['name']} ({config_idx + 1}/{len(ablation_configs)})\")\n",
        "\n",
        "    # Memory management context\n",
        "    with resource_manager.memory_managed_training():\n",
        "        # Pre-training memory stats\n",
        "        pre_memory = resource_manager.get_memory_stats()\n",
        "        print(f\"Pre-training memory: {pre_memory['allocated_mb']:.1f}MB allocated\")\n",
        "\n",
        "        # Load model with speed optimization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/git-base\",\n",
        "            torch_dtype=torch.bfloat16,  # Use bfloat16 for A100 speed\n",
        "            device_map=\"auto\" if torch.cuda.device_count() > 1 else None,\n",
        "            low_cpu_mem_usage=True,\n",
        "            use_cache=False  # Disable cache during training for memory efficiency\n",
        "        )\n",
        "        processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
        "\n",
        "        # CRITICAL: Configure tokenizer padding immediately after loading\n",
        "        configure_tokenizer_padding(processor)\n",
        "\n",
        "        # Configure LoRA with speed considerations\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,  # Increased rank for potentially better convergence with fewer epochs\n",
        "            lora_alpha=32,  # Increased alpha proportionally\n",
        "            target_modules=config[\"target_modules\"],\n",
        "            lora_dropout=0.05,  # Reduced dropout for speed\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            use_rslora=True,  # Use rank-stabilized LoRA for better training\n",
        "            use_dora=False   # Disable DoRA for speed\n",
        "        )\n",
        "\n",
        "        peft_model = get_peft_model(model, lora_config)\n",
        "        peft_model.to(device)\n",
        "\n",
        "        # Ensure gradients are enabled for trainable parameters\n",
        "        peft_model.train()\n",
        "        for name, param in peft_model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.requires_grad_(True)\n",
        "\n",
        "        peft_model.print_trainable_parameters()\n",
        "\n",
        "        # Verify gradient requirements\n",
        "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "        print(f\"Verified trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "        # Create high-performance data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=64,  # Larger batch size for speed\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,  # Enable for faster GPU transfer\n",
        "            num_workers=4,    # Parallel data loading\n",
        "            persistent_workers=True,  # Keep workers alive between epochs\n",
        "            drop_last=True,   # Avoid partial batches for consistent timing\n",
        "            prefetch_factor=2  # Prefetch batches for speed\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=64,\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,\n",
        "            num_workers=4,\n",
        "            persistent_workers=True,\n",
        "            drop_last=True\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=32,  # Larger batch for faster evaluation\n",
        "            collate_fn=collate_fn,\n",
        "            pin_memory=True,\n",
        "            num_workers=2,\n",
        "            persistent_workers=True\n",
        "        )\n",
        "\n",
        "        # Memory checkpoint after model loading\n",
        "        post_load_memory = resource_manager.get_memory_stats()\n",
        "        print(f\"Post-model-load memory: {post_load_memory['allocated_mb']:.1f}MB allocated\")\n",
        "\n",
        "        # Configure high-speed trainer\n",
        "        trainer = HighSpeedPaddingTrainer(\n",
        "            model=peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=collate_fn,\n",
        "            processing_class=processor.tokenizer,\n",
        "        )\n",
        "\n",
        "        # Compile the model for speed (PyTorch 2.0+)\n",
        "        try:\n",
        "            peft_model = torch.compile(peft_model, mode=\"reduce-overhead\")\n",
        "            print(\"Model compiled successfully for speed optimization\")\n",
        "        except Exception as e:\n",
        "            print(f\"Model compilation failed: {e}. Continuing without compilation.\")\n",
        "\n",
        "        # Additional check: Ensure model is in training mode\n",
        "        peft_model.train()\n",
        "\n",
        "        # Verify that the model has trainable parameters before training\n",
        "        trainable_count = sum(1 for p in peft_model.parameters() if p.requires_grad)\n",
        "        if trainable_count == 0:\n",
        "            raise RuntimeError(f\"No trainable parameters found for {config['name']}\")\n",
        "\n",
        "        print(f\"Ready to train with {trainable_count} parameter tensors requiring gradients\")\n",
        "\n",
        "        # Track training metrics\n",
        "        start_time = time.time()\n",
        "        memory_snapshots = []\n",
        "\n",
        "        # Training with memory monitoring\n",
        "        try:\n",
        "            print(\"Starting training...\")\n",
        "            # CRITICAL: Ensure padding is set before training starts\n",
        "            configure_tokenizer_padding(processor)\n",
        "\n",
        "            train_result = trainer.train()\n",
        "\n",
        "            # Collect training metrics\n",
        "            train_losses = [log[\"loss\"] for log in trainer.state.log_history if \"loss\" in log]\n",
        "            val_losses = [log[\"eval_loss\"] for log in trainer.state.log_history if \"eval_loss\" in log]\n",
        "\n",
        "            # Memory usage tracking (sample during training)\n",
        "            for epoch in range(min(1, len(train_losses))):  # Limit memory snapshots\n",
        "                memory_stats = resource_manager.get_memory_stats()\n",
        "                memory_snapshots.append(memory_stats['allocated_mb'])\n",
        "\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            print(\"Training completed. Starting evaluation...\")\n",
        "            # CRITICAL: Reconfigure padding before evaluation\n",
        "            configure_tokenizer_padding(processor)\n",
        "\n",
        "            # Evaluation with memory management\n",
        "            resource_manager.clear_memory()\n",
        "            test_loss, bleu_score = evaluate_model_optimized(\n",
        "                peft_model, test_loader, processor, max_length, device\n",
        "            )\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                \"experiment\": config[\"name\"],\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_losses\": val_losses,\n",
        "                \"memory_usage_MB\": memory_snapshots,\n",
        "                \"training_time_seconds\": training_time,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"bleu_score\": bleu_score,\n",
        "                \"trainable_parameters\": peft_model.get_nb_trainable_parameters()[0],\n",
        "                \"max_memory_mb\": post_load_memory['max_allocated_mb']\n",
        "            })\n",
        "\n",
        "            print(f\"Experiment completed: BLEU={bleu_score:.4f}, Test Loss={test_loss:.4f}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Training failed with error: {e}\")\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(\"Attempting recovery with smaller batch size...\")\n",
        "                # Could implement fallback strategy here\n",
        "            results.append({\n",
        "                \"experiment\": config[\"name\"],\n",
        "                \"train_losses\": [],\n",
        "                \"val_losses\": [],\n",
        "                \"memory_usage_MB\": [],\n",
        "                \"training_time_seconds\": 0,\n",
        "                \"test_loss\": float('inf'),\n",
        "                \"bleu_score\": 0,\n",
        "                \"trainable_parameters\": 0,\n",
        "                \"max_memory_mb\": 0,\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            # Aggressive cleanup after each experiment\n",
        "            print(\"Cleaning up resources...\")\n",
        "            del peft_model, model, trainer\n",
        "            del train_loader, val_loader, test_loader\n",
        "            resource_manager.clear_memory(aggressive=True)\n",
        "\n",
        "            # Final memory check\n",
        "            final_memory = resource_manager.get_memory_stats()\n",
        "            print(f\"Post-cleanup memory: {final_memory['allocated_mb']:.1f}MB allocated\")\n",
        "\n",
        "# Save results with additional memory metrics\n",
        "folder_path = '/content/drive/MyDrive/GIT AblationStratergy/ablationStudy/results/LoRA'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(os.path.join(folder_path, 'lora_target_modules_ablation_results_optimized.csv'), index=False)\n",
        "\n",
        "print(\"\\nAblation Study Results (Target Modules) - A100 Optimized:\")\n",
        "display_columns = [\"experiment\", \"test_loss\", \"bleu_score\", \"training_time_seconds\", \"trainable_parameters\", \"max_memory_mb\"]\n",
        "print(results_df[display_columns])\n",
        "\n",
        "# Memory usage summary\n",
        "total_experiments = len([r for r in results if 'error' not in r])\n",
        "print(f\"\\nResource Management Summary:\")\n",
        "print(f\"- Completed experiments: {total_experiments}/{len(ablation_configs)}\")\n",
        "print(f\"- Peak memory usage: {max([r.get('max_memory_mb', 0) for r in results]):.1f}MB\")\n",
        "print(f\"- Average training time: {sum([r['training_time_seconds'] for r in results if r['training_time_seconds'] > 0]) / max(total_experiments, 1):.1f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OVD6T5KLl2q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM0R7FqDuBucJK8xahW2Z7J",
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "1pRfsPQlDWevcT3yFRac9jUT8RginW9m3",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
